{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ff415ba-21a5-4b9a-b25b-0cf1c1fb05d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "# from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "datPath = '/root/autodl-tmp/BertClassNews/data/'\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c7ccfab-cd8b-438b-aa99-576f6120fa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Trpath = datPath + 'train_cls-sample.txt'\n",
    "Testpath = datPath + 'dev_cls-sample.txt'\n",
    "\n",
    "# f = open(pathTr, 'r')\n",
    "# train_val = f.read()\n",
    "\n",
    "def read_process_dat(path):\n",
    "    try:\n",
    "        with open(path) as f:\n",
    "            text = f.read()\n",
    "    except UnicodeDecodeError:        \n",
    "        with open(path, errors='ignore') as f:\n",
    "            text = f.read()\n",
    "    pattern = r'[\\t\\n]'\n",
    "    result = re.split(pattern, text)\n",
    "    contentIndx = 0\n",
    "    lblIndx = 1\n",
    "    n = len(result) - 1\n",
    "    content, label = [], []\n",
    "    while lblIndx < n:\n",
    "        content.append(result[contentIndx])\n",
    "        label.append(result[lblIndx])\n",
    "        contentIndx += 2\n",
    "        lblIndx += 2\n",
    "    df = pd.DataFrame()\n",
    "    df['tag'] = label\n",
    "    df['content'] = content\n",
    "    df['len'] = df['content'].apply(lambda x: sum([i.isalpha() for i in x]))\n",
    "    df = df[df['len']>27]\n",
    "    return df\n",
    "\n",
    "train_val_df = read_process_dat(Trpath)\n",
    "test_df = read_process_dat(Testpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84b13c7d-4fd3-416a-ba24-09e7fe39615c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32751, 3),\n",
       " (4532, 3),\n",
       " Counter({'政策类型': 2557,\n",
       "          '基建民生': 6398,\n",
       "          '社会热点': 8704,\n",
       "          '人事任免': 2167,\n",
       "          '产业金融': 1135,\n",
       "          '本地旅游': 6880,\n",
       "          '通报查处': 3097,\n",
       "          '数据排名': 799,\n",
       "          '暖新闻': 680,\n",
       "          '人文历史': 334}),\n",
       " Counter({'数据排名': 111,\n",
       "          '社会热点': 1205,\n",
       "          '基建民生': 889,\n",
       "          '本地旅游': 936,\n",
       "          '政策类型': 370,\n",
       "          '人事任免': 280,\n",
       "          '通报查处': 436,\n",
       "          '暖新闻': 95,\n",
       "          '产业金融': 171,\n",
       "          '人文历史': 39}))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val_df.shape, test_df.shape, Counter(train_val_df['tag'].tolist()), Counter(test_df['tag'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efe1e37f-02e7-4d92-9267-47227903a1dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_val_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5090/798014031.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_val_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_val_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tag'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatPath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'train_cls.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mval_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatPath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'val_cls.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_val_df' is not defined"
     ]
    }
   ],
   "source": [
    "# train valid split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.2, stratify=train_val_df['tag'], random_state=42)\n",
    "train_df.to_json(datPath + 'train_cls.json')\n",
    "val_df.to_json(datPath + 'val_cls.json')\n",
    "test_df.to_json(datPath + 'test_cls.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2bb538-212d-4bf0-8c17-99914c7e12cf",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bd1249f-e8b0-484d-9f35-b19e774b504b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "datPath = '/root/autodl-tmp/BertClassNews/data/'\n",
    "wtPath = '/root/autodl-tmp/BertClassNews/wt/'\n",
    "\n",
    "train_df = pd.read_json(datPath + 'train_cls.json')\n",
    "val_df = pd.read_json(datPath + 'val_cls.json')\n",
    "test_df = pd.read_json(datPath + 'test_cls.json')\n",
    "\n",
    "# train_df = pd.read_json(datPath + 'smpl_train.json')\n",
    "# val_df = pd.read_json(datPath + 'smpl_val.json')\n",
    "# test_df = pd.read_json(datPath + 'smpl_test.json')\n",
    "\n",
    "lblEncode = {j:i for i,j in zip(range(len(train_df['tag'].unique())), train_df['tag'].unique())}\n",
    "train_df['tag'] = train_df['tag'].map(lblEncode)\n",
    "val_df['tag'] = val_df['tag'].map(lblEncode)\n",
    "test_df['tag'] = test_df['tag'].map(lblEncode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b086a0c-3af0-4dc8-b4e5-fad0e87394b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a38926-edfe-427b-9605-bfa19370ef85",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "947775e2-6166-4cd8-9f8c-fd91e416bfb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer, BertForSequenceClassification\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mBertTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbert-base-chinese\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# lblEncode = {j:i for i,j in zip(range(len(train_df['tag'].unique())), train_df['tag'].unique())}\u001b[39;00m\n\u001b[1;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m BertForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-chinese\u001b[39m\u001b[38;5;124m'\u001b[39m,num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(lblEncode))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1813\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1811\u001b[0m             resolved_vocab_files[file_id] \u001b[38;5;241m=\u001b[39m download_url(file_path, proxies\u001b[38;5;241m=\u001b[39mproxies)\n\u001b[1;32m   1812\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1813\u001b[0m         resolved_vocab_files[file_id] \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1814\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1815\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1816\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1817\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1818\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1819\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1820\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1821\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1822\u001b[0m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1823\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1824\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1825\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1826\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1827\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1828\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1829\u001b[0m         commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_vocab_files[file_id], commit_hash)\n\u001b[1;32m   1831\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unresolved_files) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/utils/hub.py:429\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    426\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 429\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    445\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to request access at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and pass a token having permission to this repo either \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    448\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/huggingface_hub/file_download.py:1230\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, endpoint, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1229\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1230\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1231\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1232\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1236\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[1;32m   1237\u001b[0m         \u001b[38;5;66;03m# Cache the non-existence of the file and raise\u001b[39;00m\n\u001b[1;32m   1238\u001b[0m         commit_hash \u001b[38;5;241m=\u001b[39m http_error\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(HUGGINGFACE_HEADER_X_REPO_COMMIT)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/huggingface_hub/file_download.py:1597\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[1;32m   1594\u001b[0m headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccept-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midentity\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# prevent any compression => we want to know the real size of the file\u001b[39;00m\n\u001b[1;32m   1596\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1597\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1598\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1599\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1601\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1602\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1603\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1605\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1606\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m   1608\u001b[0m \u001b[38;5;66;03m# Return\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/huggingface_hub/file_download.py:417\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;66;03m# 2. Force relative redirection\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 417\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_wait_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_wait_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_wait_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_wait_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m300\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m399\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/huggingface_hub/file_download.py:452\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m    451\u001b[0m \u001b[38;5;66;03m# 3. Exponential backoff\u001b[39;00m\n\u001b[0;32m--> 452\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhttp_backoff\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_wait_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_wait_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_wait_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_wait_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry_on_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mTimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mProxyError\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry_on_status_codes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/huggingface_hub/utils/_http.py:258\u001b[0m, in \u001b[0;36mhttp_backoff\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mseek(io_obj_initial_pos)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 258\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m retry_on_status_codes:\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/huggingface_hub/utils/_http.py:63\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Catch any RequestException to append request id to the error message for debugging.\"\"\"\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     65\u001b[0m     request_id \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m         resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m            \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m            \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(conn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproxy_pool\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/urllib3/connectionpool.py:699\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    696\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    698\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 699\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    713\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/urllib3/connectionpool.py:382\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 382\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;66;03m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mconn\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/urllib3/connectionpool.py:1010\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(conn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msock\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[0;32m-> 1010\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified:\n\u001b[1;32m   1013\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1014\u001b[0m         (\n\u001b[1;32m   1015\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnverified HTTPS request is being made to host \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1020\u001b[0m         InsecureRequestWarning,\n\u001b[1;32m   1021\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/urllib3/connection.py:353\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;66;03m# Add certificate verification\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m     hostname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n\u001b[1;32m    355\u001b[0m     tls_in_tls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/urllib3/connection.py:169\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    166\u001b[0m     extra_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msocket_options\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket_options\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kw\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeoutError(\n\u001b[1;32m    175\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m timed out. (connect timeout=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    177\u001b[0m         \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout),\n\u001b[1;32m    178\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/urllib3/util/connection.py:86\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m source_address:\n\u001b[1;32m     85\u001b[0m         sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 86\u001b[0m     \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sock\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39merror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# !pip install torch transformers\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "# lblEncode = {j:i for i,j in zip(range(len(train_df['tag'].unique())), train_df['tag'].unique())}\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-chinese',num_labels=len(lblEncode))\n",
    "# inputs = tokenizer(text, return_tensors='pt', padding=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "088a11a6-e34e-4fa0-862a-7722ade9c64d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '本地旅游',\n",
       " 1: '通报查处',\n",
       " 2: '基建民生',\n",
       " 3: '社会热点',\n",
       " 4: '暖新闻',\n",
       " 5: '人事任免',\n",
       " 6: '政策类型',\n",
       " 7: '产业金融',\n",
       " 8: '人文历史',\n",
       " 9: '数据排名'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_lblEncode = {lblEncode[i]:i for i in lblEncode}\n",
    "reverse_lblEncode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2759353a-7798-48fa-89b4-1ddc957d7e4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'本地旅游': 0,\n",
       " '通报查处': 1,\n",
       " '基建民生': 2,\n",
       " '社会热点': 3,\n",
       " '暖新闻': 4,\n",
       " '人事任免': 5,\n",
       " '政策类型': 6,\n",
       " '产业金融': 7,\n",
       " '人文历史': 8,\n",
       " '数据排名': 9}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lblEncode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcf35ae8-9d7c-46d6-8d49-fabafeee289a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import sklearn.model_selection\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61c3aa05-fc25-4ecd-b4de-6adccaf940ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reverse_lblEncode = {lblEncode[i]:i for i in lblEncode}\n",
    "\n",
    "class BERTDataset(Dataset):\n",
    "\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.inputs.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_texts, val_texts, test_texts = train_df['content'].tolist(), val_df['content'].tolist(), test_df['content'].tolist()\n",
    "train_labels, val_labels, test_labels = train_df['tag'].tolist(), val_df['tag'].tolist(), test_df['tag'].tolist()\n",
    "\n",
    "# train_texts, val_texts, test_texts = train_df['content'].iloc[:100].tolist(), val_df['content'].iloc[:100].tolist(), test_df['content'].iloc[:100].tolist()\n",
    "# train_labels, val_labels, test_labels = train_df['tag'].iloc[:100].tolist(), val_df['tag'].iloc[:100].tolist(), test_df['tag'].iloc[:100].tolist()\n",
    "def generateDataloader(df, lbl, BATCH_SIZE = 16):\n",
    "    inputs = tokenizer(df, padding=True, truncation=True,return_tensors='pt')\n",
    "    dataset = BERTDataset(inputs, lbl)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "    return dataloader\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    final_pred = []\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    pred = {i:[] for i in range(len(lblEncode))}\n",
    "    # pred = {i:[] for i in [0,2,3]}\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch in dataloader:\n",
    "            inputs = torch.stack([t for t in batch['input_ids']]).to(device)\n",
    "            labels = torch.tensor(batch['labels']).to(device)\n",
    "            attention_mask =torch.stack([t for t in batch['attention_mask']]).to(device)\n",
    "            outputs = model(input_ids=inputs, attention_mask=attention_mask,\n",
    "                      labels=labels)\n",
    "            predictions = outputs[1].argmax(dim=1)\n",
    "            for i in range(len(predictions)):\n",
    "                predRes = predictions[i].item()\n",
    "                pred[predRes].append((predictions[i] == labels[i]).item())\n",
    "                final_pred.append((reverse_lblEncode[predRes], reverse_lblEncode[labels[i].item()]))\n",
    "                # final_pred.append((predRes, labels[i].item()))\n",
    "\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.shape[0]\n",
    "      \n",
    "    accuracy = correct/total\n",
    "    res = {reverse_lblEncode[i]:sum(pred[i])/len(pred[i]) if len(pred[i]) > 0 else np.nan for i in pred }\n",
    "  # res = {i:sum(pred[i])/len(pred[i]) if len(pred[i]) > 0 else np.nan for i in pred }\n",
    "\n",
    "    return accuracy, res, final_pred\n",
    "\n",
    "def model_train(model, train_dataloader, val_dataloader, iter = 3):\n",
    "    for epoch in range(3):\n",
    "      start = time.time()\n",
    "      losses = 0\n",
    "      model.train()\n",
    "      idx = 0\n",
    "      for batch in train_dataloader:\n",
    "        inputs, token_type_ids, attention_mask, labels = batch['input_ids'], batch['token_type_ids'], batch['attention_mask'], batch['labels']\n",
    "\n",
    "        input_ids = torch.stack([t for t in batch['input_ids']]).to(device)\n",
    "\n",
    "        attention_mask =torch.stack([t for t in batch['attention_mask']]).to(device)\n",
    "        labels = torch.tensor(batch['labels']).to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask,labels=labels)\n",
    "        loss = criterion(outputs['logits'],labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        losses += loss.item()\n",
    "        acc = (outputs['logits'].argmax(1) == labels).float().mean()\n",
    "        idx += 1\n",
    "        curr = time.time()\n",
    "        timeConsume = curr - start\n",
    "        if idx % 300 == 0:\n",
    "            print (f'Epoch {epoch+1}, batch {idx}, so far takes {timeConsume}')\n",
    "          # los  =outputs['loss']\n",
    "            print (f'Loss: {losses}')\n",
    "\n",
    "      print (acc)\n",
    "      val_acc, val_cat_acc, val_final_pred= evaluate(model, val_dataloader)\n",
    "      print ('val acc:', val_acc)\n",
    "        # if idx % 10 == 0:\n",
    "        #     logging.info(f\"Epoch: {epoch}, Batch[{idx}/{len(train_iter)}], \"\n",
    "        #                   f\"Train loss :{loss.item():.3f}, Train acc: {acc:.3f}\")\n",
    "\n",
    "      print(f'Epoch {epoch+1} complete, so far takes {timeConsume}')\n",
    "    return model\n",
    "\n",
    "\n",
    "def save_model(model, datPath, model_name = 'BertOrigmodelAllDat_v0.pt'):\n",
    "    # datPath = '/root/wt/'\n",
    "    # model_save_path  = datPath + 'BertOrigmodelAllDat_v0.pt'\n",
    "    model_save_path  = datPath + model_name\n",
    "    if os.path.exists(model_save_path):\n",
    "        loaded_paras = torch.load(model_save_path)\n",
    "        model.load_state_dict(loaded_paras)\n",
    "        logging.info(\"## 成功载入已有模型，进行追加训练......\")\n",
    "    else:\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "44a6a14d-a17c-4434-92bb-1ed1c3a31209",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = save_model(model, datPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d003a6b-c0d2-4d4c-967f-eebb7b173a38",
   "metadata": {},
   "source": [
    "# Retrain on the new training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969ddf5a-4e12-4535-a328-4211b70f6341",
   "metadata": {},
   "source": [
    "## Prepare for new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f55fcb9-be5a-4ede-949b-d508fc21fb49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader = generateDataloader(train_df['content'].tolist(), train_df['tag'].tolist()) \n",
    "val_dataloader = generateDataloader(val_df['content'].tolist(), val_df['tag'].tolist()) \n",
    "test_dataloader = generateDataloader(test_df['content'].tolist(), test_df['tag'].tolist()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a15d9af5-eefb-4baa-ab4c-7e62c6d752f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.08729007633587786 Train Cat Accuracy: {'本地旅游': 0.06707317073170732, '通报查处': 0.0, '基建民生': 0.061068702290076333, '社会热点': 0.6525522041763341, '暖新闻': 0.0, '人事任免': 0.038632045598480054, '政策类型': 0.08110578021475896, '产业金融': 0.040842696629213485, '人文历史': 0.0, '数据排名': nan}\n"
     ]
    }
   ],
   "source": [
    "train_accuracy, train_cat_acc, train_pred = evaluate(model, train_dataloader)\n",
    "print(\"Train Accuracy:\", train_accuracy, \"Train Cat Accuracy:\", train_cat_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d957f7f7-0b3a-48a4-9538-bf99a86ae265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, batch 300, so far takes 102.73140144348145\n",
      "Loss: 62.12770641036332\n",
      "Epoch 1, batch 600, so far takes 205.50254940986633\n",
      "Loss: 121.30345359444618\n",
      "Epoch 1, batch 900, so far takes 308.28847217559814\n",
      "Loss: 171.19219447858632\n",
      "Epoch 1, batch 1200, so far takes 411.175475358963\n",
      "Loss: 222.6315535181202\n",
      "Epoch 1, batch 1500, so far takes 514.284298658371\n",
      "Loss: 268.84038855461404\n",
      "tensor(1., device='cuda:0')\n",
      "val acc: 0.8923828423141505\n",
      "Epoch 1 complete, so far takes 561.3614139556885\n",
      "Epoch 2, batch 300, so far takes 102.96962356567383\n",
      "Loss: 62.883980221580714\n",
      "Epoch 2, batch 600, so far takes 205.7737991809845\n",
      "Loss: 120.75829038955271\n",
      "Epoch 2, batch 900, so far takes 308.98069047927856\n",
      "Loss: 168.42091518919915\n",
      "Epoch 2, batch 1200, so far takes 411.88878774642944\n",
      "Loss: 213.88372763851658\n",
      "Epoch 2, batch 1500, so far takes 514.7534019947052\n",
      "Loss: 255.612309363205\n",
      "tensor(1., device='cuda:0')\n",
      "val acc: 0.8920775454129141\n",
      "Epoch 2 complete, so far takes 561.9217591285706\n",
      "Epoch 3, batch 300, so far takes 102.76760387420654\n",
      "Loss: 46.898291625780985\n",
      "Epoch 3, batch 600, so far takes 205.69826912879944\n",
      "Loss: 92.1190067473799\n",
      "Epoch 3, batch 900, so far takes 308.44861340522766\n",
      "Loss: 130.47419500211254\n",
      "Epoch 3, batch 1200, so far takes 411.52451395988464\n",
      "Loss: 170.03525681979954\n",
      "Epoch 3, batch 1500, so far takes 514.3225336074829\n",
      "Loss: 202.7320296476828\n",
      "tensor(0.8750, device='cuda:0')\n",
      "val acc: 0.8943672721721875\n",
      "Epoch 3 complete, so far takes 561.777735710144\n"
     ]
    }
   ],
   "source": [
    "new_model = model_train(new_model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "543a6808-3264-4143-a67d-0ba3340af7ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_model = save_model(model, wtPath, model_name = 'BertRetrainNewAllDat_v1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7965458a-6ab4-4742-b5cb-3cb07dbf0c33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9638549618320611 Train Cat Accuracy: {'本地旅游': 0.9931073025335321, '通报查处': 0.9350550702620585, '基建民生': 0.9524882629107981, '社会热点': 0.9898974892289407, '暖新闻': 0.8781302170283807, '人事任免': 0.9832949308755761, '政策类型': 0.9753747323340471, '产业金融': 0.9157549234135668, '人文历史': 0.7053824362606232, '数据排名': 0.8677563150074294}\n"
     ]
    }
   ],
   "source": [
    "train_accuracy, train_cat_acc, train_pred = evaluate(new_model, train_dataloader)\n",
    "print(\"Train Accuracy:\", train_accuracy, \"Train Cat Accuracy:\", train_cat_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a8f86908-bce1-4f46-a319-3031ac864abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 0.8943672721721875 Val Cat Accuracy: {'本地旅游': 0.9584592145015106, '通报查处': 0.8989598811292719, '基建民生': 0.864530225782957, '社会热点': 0.9170644391408115, '暖新闻': 0.6956521739130435, '人事任免': 0.9837209302325581, '政策类型': 0.9168490153172867, '产业金融': 0.7703349282296651, '人文历史': 0.422680412371134, '数据排名': 0.6954022988505747}\n"
     ]
    }
   ],
   "source": [
    "val_accuracy, val_cat_acc, val_pred = evaluate(new_model, val_dataloader)\n",
    "print(\"Val Accuracy:\", val_accuracy, \"Val Cat Accuracy:\", val_cat_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e1b940ae-9b82-41ca-bfe1-c02dd9b6304e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8826125330979699 Test Cat Accuracy: {'本地旅游': 0.9641255605381166, '通报查处': 0.8914893617021277, '基建民生': 0.8492723492723493, '社会热点': 0.9102112676056338, '暖新闻': 0.6333333333333333, '人事任免': 0.9675090252707581, '政策类型': 0.8978328173374613, '产业金融': 0.7604790419161677, '人文历史': 0.423728813559322, '数据排名': 0.6666666666666666}\n"
     ]
    }
   ],
   "source": [
    "test_accuracy, test_cat_acc, test_pred = evaluate(new_model, test_dataloader)\n",
    "print(\"Test Accuracy:\", test_accuracy, \"Test Cat Accuracy:\", test_cat_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaec9262-70b4-4a6a-91ab-18e825cf3b4b",
   "metadata": {},
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3dd62a1f-62e4-4cc2-b99a-10e13a2923a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "def remove_part_fPred_train(train_df, train_pred):\n",
    "    cond_acc = []\n",
    "    for i,j in train_pred:\n",
    "        cond_acc.append((i==j) |((i!=j)&(j not in ['本地旅游','基建民生','社会热点'])))\n",
    "    # cond_acc = pd.Series(cond_acc)\n",
    "    new_train = train_df[cond_acc]\n",
    "    new_train = new_train.reset_index(drop = True)\n",
    "    return new_train\n",
    "\n",
    "def augment_minority_randomSample(train_df, augment_rate):\n",
    "    oversample = RandomOverSampler(sampling_strategy = augment_rate)\n",
    "    X_over, y_over = oversample.fit_resample(train_df.drop('tag', axis=1), train_df['tag'])\n",
    "    X_over['tag'] = y_over\n",
    "    return X_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c7c57a1-69d8-4287-bed1-c85a7f837135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_model = save_model(model, wtPath, model_name = 'BertRetrainNewAllDat_v1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96f6d136-72bd-427d-90a1-df1b3b084571",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_train = remove_part_fPred_train(train_df, train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b71d8f7-9b94-4f6d-baee-b69b47fecd34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_train.to_json(datPath + 'new_train_cls.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b1f863c-5269-48a3-bc8a-584c3d519041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({0: 5331,\n",
       "          1: 2477,\n",
       "          2: 5072,\n",
       "          3: 6663,\n",
       "          4: 544,\n",
       "          5: 1734,\n",
       "          6: 2046,\n",
       "          7: 908,\n",
       "          8: 267,\n",
       "          9: 639}),\n",
       " Counter({0: 5504,\n",
       "          1: 2477,\n",
       "          2: 5118,\n",
       "          3: 6963,\n",
       "          4: 544,\n",
       "          5: 1734,\n",
       "          6: 2046,\n",
       "          7: 908,\n",
       "          8: 267,\n",
       "          9: 639}),\n",
       " {0: '本地旅游',\n",
       "  1: '通报查处',\n",
       "  2: '基建民生',\n",
       "  3: '社会热点',\n",
       "  4: '暖新闻',\n",
       "  5: '人事任免',\n",
       "  6: '政策类型',\n",
       "  7: '产业金融',\n",
       "  8: '人文历史',\n",
       "  9: '数据排名'})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(new_train['tag'].tolist()), Counter(train_df['tag'].tolist()), reverse_lblEncode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "791e9309-fc8b-4940-bbaa-f386386c2a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9833339823215607 Train Cat Accuracy: {'本地旅游': 0.9932923420905534, '通报查处': 0.9386199008768585, '基建民生': 0.9956811935610522, '社会热点': 0.9970073320365106, '暖新闻': 1.0, '人事任免': 0.9832949308755761, '政策类型': 0.9795698924731183, '产业金融': 0.9393939393939394, '人文历史': 0.9920318725099602, '数据排名': 0.8984615384615384}\n"
     ]
    }
   ],
   "source": [
    "new_train_dataloader = generateDataloader(new_train['content'].tolist(), new_train['tag'].tolist()) \n",
    "train_accuracy, train_cat_acc, train_pred = evaluate(new_model, new_train_dataloader)\n",
    "print(\"Train Accuracy:\", train_accuracy, \"Train Cat Accuracy:\", train_cat_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "469eb679-29af-4863-9560-4686493b70dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model2 = model_train(new_model, new_train_dataloader, val_dataloader)\n",
    "test_accuracy, test_cat_acc, test_pred = evaluate(new_model2, test_dataloader)\n",
    "print(\"Test Accuracy:\", test_accuracy, \"Test Cat Accuracy:\", test_cat_acc)\n",
    "new_model2 = save_model(new_model2, '/root/wt/', model_name = 'BertRetrainNewAllDatRemErrTrain_v2.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0732c5-900f-4f57-8d15-f1ff09f3b671",
   "metadata": {},
   "source": [
    "## Random upsampling to minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "850dabff-aaad-4ea1-9257-4a330eab7bb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "augment_rate2 = {0: 5331,1: 2477,2: 5072,3: 6663,4: 544 * 2,5: 1734,6: 2046,7: 908*2,8: 267*3,9: 639*2}\n",
    "# new_train = remove_part_fPred_train(train_df, train_pred)\n",
    "new_train = pd.read_json(datPath + 'new_train_cls.json')\n",
    "new_train = augment_minority_randomSample(new_train, augment_rate2)\n",
    "new_train_dataloader = generateDataloader(new_train['content'].tolist(), new_train['tag'].tolist()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ae14fe81-501a-4594-9339-4aee32e05cd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 5331,\n",
       "         1: 2477,\n",
       "         2: 5072,\n",
       "         3: 6663,\n",
       "         4: 1632,\n",
       "         5: 1734,\n",
       "         6: 2046,\n",
       "         7: 1816,\n",
       "         8: 801,\n",
       "         9: 1917})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(new_train['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5024e5-7acb-4595-bb01-3a493ff7cdcc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, batch 300, so far takes 103.42347836494446\n",
      "Loss: 231.57247819006443\n",
      "Epoch 1, batch 600, so far takes 207.21534299850464\n",
      "Loss: 376.84316881000996\n",
      "Epoch 1, batch 900, so far takes 310.8795609474182\n",
      "Loss: 483.290787499398\n",
      "Epoch 1, batch 1200, so far takes 414.6867365837097\n",
      "Loss: 588.0966288074851\n",
      "Epoch 1, batch 1500, so far takes 518.4460413455963\n",
      "Loss: 673.5722362883389\n",
      "tensor(1., device='cuda:0')\n",
      "val acc: 0.024576400549534423\n",
      "Epoch 1 complete, so far takes 611.8735513687134\n"
     ]
    }
   ],
   "source": [
    "new_model3 = model_train(model, new_train_dataloader, val_dataloader)\n",
    "test_accuracy, test_cat_acc, test_pred = evaluate(new_model2, test_dataloader)\n",
    "print(\"Test Accuracy:\", test_accuracy, \"Test Cat Accuracy:\", test_cat_acc)\n",
    "new_model3 = save_model(new_model3, wtPath, model_name = 'BertRetrainNewAllDatRemErrTrainRandUp_v3.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4dd562-f1bf-4135-ade8-833273b986da",
   "metadata": {},
   "source": [
    "## Random upsampling to minority class. Retrain for BertRetrainNewAllDat_v1.pt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2d561f-a4df-47a2-97ae-949c12e71369",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, batch 300, so far takes 99.83909177780151\n",
      "Loss: 697.2860451936722\n",
      "Epoch 1, batch 600, so far takes 200.14200615882874\n",
      "Loss: 1391.9564501047134\n",
      "Epoch 1, batch 900, so far takes 300.4501922130585\n",
      "Loss: 2089.886576652527\n",
      "Epoch 1, batch 1200, so far takes 400.8224024772644\n",
      "Loss: 2784.9920345544815\n",
      "Epoch 1, batch 1500, so far takes 501.2716443538666\n",
      "Loss: 3483.6392674446106\n",
      "tensor(0.5000, device='cuda:0')\n",
      "val acc: 0.2544649671805831\n",
      "Epoch 1 complete, so far takes 591.6769473552704\n",
      "Epoch 2, batch 300, so far takes 100.44147372245789\n",
      "Loss: 699.0497311353683\n",
      "Epoch 2, batch 600, so far takes 200.7929425239563\n",
      "Loss: 1395.0247613191605\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "new_model_v2 = model_train(model, new_train_dataloader, val_dataloader)\n",
    "test_accuracy, test_cat_acc, test_pred = evaluate(new_model_v2, test_dataloader)\n",
    "print(\"Test Accuracy:\", test_accuracy, \"Test Cat Accuracy:\", test_cat_acc)\n",
    "# new_model3 = save_model(new_model3, '/root/wt/', model_name = 'BertRetrainNewAllDatRemErrTrainRandUp_v3.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eff80a8-f935-418c-b9d6-b7f6df2c617b",
   "metadata": {},
   "source": [
    "# Train from scratch on augmented dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d23df2-a945-47b2-81bf-17693324d55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_model = save_model(model, '/root/wt/', model_name = 'BertRetrainNewAllDat_v1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "315fe4a9-7035-445e-b198-1ee734942087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, batch 300, so far takes 99.42090773582458\n",
      "Loss: 705.8370118141174\n",
      "Epoch 1, batch 600, so far takes 199.7743377685547\n",
      "Loss: 1411.5514986515045\n",
      "Epoch 1, batch 900, so far takes 300.2780954837799\n",
      "Loss: 2115.0702583789825\n",
      "Epoch 1, batch 1200, so far takes 400.8581645488739\n",
      "Loss: 2818.7002758979797\n",
      "Epoch 1, batch 1500, so far takes 501.4085969924927\n",
      "Loss: 3521.4392290115356\n",
      "Epoch 1, batch 1800, so far takes 602.1666820049286\n",
      "Loss: 4259.40271115303\n",
      "tensor(0., device='cuda:0')\n",
      "val acc: 0.11891314303159822\n",
      "Epoch 1 complete, so far takes 616.7841415405273\n",
      "Epoch 2, batch 300, so far takes 100.55902981758118\n",
      "Loss: 707.7108061313629\n",
      "Epoch 2, batch 600, so far takes 201.13754200935364\n",
      "Loss: 1414.274089217186\n",
      "Epoch 2, batch 900, so far takes 301.71672534942627\n",
      "Loss: 2119.3740879297256\n",
      "Epoch 2, batch 1200, so far takes 402.30867624282837\n",
      "Loss: 2821.7742491960526\n",
      "Epoch 2, batch 1500, so far takes 502.8005893230438\n",
      "Loss: 3524.414420723915\n",
      "Epoch 2, batch 1800, so far takes 603.588666677475\n",
      "Loss: 4261.867257714272\n",
      "tensor(0., device='cuda:0')\n",
      "val acc: 0.11891314303159822\n",
      "Epoch 2 complete, so far takes 618.1945576667786\n",
      "Epoch 3, batch 300, so far takes 100.49669122695923\n",
      "Loss: 707.8818204402924\n",
      "Epoch 3, batch 600, so far takes 200.91649627685547\n",
      "Loss: 1413.5072413682938\n",
      "Epoch 3, batch 900, so far takes 301.4360001087189\n",
      "Loss: 2116.9174877405167\n",
      "Epoch 3, batch 1200, so far takes 402.0047221183777\n",
      "Loss: 2821.6642550230026\n",
      "Epoch 3, batch 1500, so far takes 502.5118713378906\n",
      "Loss: 3526.145276427269\n",
      "Epoch 3, batch 1800, so far takes 603.2379400730133\n",
      "Loss: 4263.340965628624\n",
      "tensor(0., device='cuda:0')\n",
      "val acc: 0.11891314303159822\n",
      "Epoch 3 complete, so far takes 617.8429327011108\n",
      "Test Accuracy: 0.11540158870255958 Test Cat Accuracy: {'本地旅游': 0.11372549019607843, '通报查处': 0.03169014084507042, '基建民生': 0.0, '社会热点': 0.2357263990955342, '暖新闻': 0.0, '人事任免': nan, '政策类型': nan, '产业金融': 0.015468607825295723, '人文历史': 0.03333333333333333, '数据排名': 0.012345679012345678}\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "new_model4 = model_train(model, new_train_dataloader, val_dataloader)\n",
    "test_accuracy, test_cat_acc, test_pred = evaluate(new_model4, test_dataloader)\n",
    "print(\"Test Accuracy:\", test_accuracy, \"Test Cat Accuracy:\", test_cat_acc)\n",
    "new_model4 = save_model(new_model4, '/root/wt/', model_name = 'BertRetrainNewAllDatRemErrTrainRandUp_v4.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e8adb6-a16b-4b88-91cb-001efefb1845",
   "metadata": {},
   "source": [
    "## Random nlpcda to minority class. Retrain for BertRetrainNewAllDat_v1.pt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "308e4dfc-975c-4c7c-9a50-499613e2b3b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Collecting nlpcda\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/71/6c/a485fcaf573db12b5af5b4468883c0f67e37b36e7ea0ffb502ffef646a4e/nlpcda-2.5.8.tar.gz (527 kB)\n",
      "\u001b[K     |████████████████████████████████| 527 kB 2.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jieba\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/c6/cb/18eeb235f833b726522d7ebed54f2278ce28ba9438e3135ab0278d9792a2/jieba-0.42.1.tar.gz (19.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 19.2 MB 123.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /root/miniconda3/lib/python3.8/site-packages (from nlpcda) (2.28.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.8/site-packages (from requests->nlpcda) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /root/miniconda3/lib/python3.8/site-packages (from requests->nlpcda) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.8/site-packages (from requests->nlpcda) (2021.5.30)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.8/site-packages (from requests->nlpcda) (3.1.0)\n",
      "Building wheels for collected packages: nlpcda, jieba\n",
      "  Building wheel for nlpcda (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nlpcda: filename=nlpcda-2.5.8-py3-none-any.whl size=527645 sha256=62c11ac377c3dfc7ae79d59bf308efaa56fa1da21bf1e6a8b1bf098fb88428c5\n",
      "  Stored in directory: /root/.cache/pip/wheels/fa/14/ff/34ee134516d01377323c19a6be48d77c082a8c8c623283e80a\n",
      "  Building wheel for jieba (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314477 sha256=3cf47c7ff6ab0554a67d3870aca937770186188b088eca5c99b73f56b814b45e\n",
      "  Stored in directory: /root/.cache/pip/wheels/e1/4b/67/93eb050406bb5774e9537785446258da898a98e92e7e1bbfae\n",
      "Successfully built nlpcda jieba\n",
      "Installing collected packages: jieba, nlpcda\n",
      "Successfully installed jieba-0.42.1 nlpcda-2.5.8\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nlpcda\n",
    "# from nlpcda.tools.Simbert import Simbert\n",
    "# from nlpcda import EDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d1669d4-e282-48e2-81ac-6b04f443bc3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load :/root/miniconda3/lib/python3.8/site-packages/nlpcda/data/company.txt done\n",
      "load :/root/miniconda3/lib/python3.8/site-packages/nlpcda/data/同义词.txt done\n",
      "load :/root/miniconda3/lib/python3.8/site-packages/nlpcda/data/同音意字.txt done\n",
      "load :/root/miniconda3/lib/python3.8/site-packages/nlpcda/data/等价字.txt done\n",
      "随机实体替换>>>>>>\n",
      "这是个实体：58同城；今天是2020年3月8日11:40，天气晴朗，天气很不错，空气很好，不差；这个nlpcad包，用于方便一键数据增强，可有效增强NLP模型的泛化性能、减少波动、抵抗对抗攻击\n",
      "这是个实体：中国通信服务；今天是2020年3月8日11:40，天气晴朗，天气很不错，空气很好，不差；这个nlpcad包，用于方便一键数据增强，可有效增强NLP模型的泛化性能、减少波动、抵抗对抗攻击\n",
      "这是个实体：瑞丰光电；今天是2020年3月8日11:40，天气晴朗，天气很不错，空气很好，不差；这个nlpcad包，用于方便一键数据增强，可有效增强NLP模型的泛化性能、减少波动、抵抗对抗攻击\n",
      "随机近义词替换>>>>>>\n",
      "这是个实体：58同城；今天是2020年3月8日11:40，天气晴朗，天气很不错，空气很好，不差；这个nlpcad包，用于方便一键数据增强，可有效增强NLP模型的泛化性能、减少波动、抵抗对抗攻击\n",
      "这是个实体：58同城；今儿个是2020年3月8日11:40，天气晴朗，天气很不错，空气很好，不差；夫nlpcad包，用来方便一键数据如虎添翼，可行得通加强NLP模型的泛化性质、减少游走不定、抵抗分庭抗礼攻击\n",
      "这是个实体：58同城；今天是2020年3月8日11:40，天气晴朗，天气很不错，空气很好，不差；这个nlpcad包，用于方便一键数据增强，可有效增强NLP模的泛化性、减少波动、抵抗对抗攻击\n",
      "随机近义字替换>>>>>>\n",
      "这是个实体：58同城；今天是2020年3月8日11:40，天气晴朗，天气很不错，空气很好，不差；这个nlpcad包，用于方便一键数据增强，可有效增强NLP模型的泛化性能、减少波动、抵抗对抗攻击\n",
      "这是个实体：58同城；今填是2020年3月8日11:40，天气晴朗，天气很不错，空气痕好，不差；这个nlpcad包，用于方便一键数据增强，可有效增强NLP模型的泛化性能、减少波动、抵抗对抗攻击\n",
      "鷓是个实体：58同乘；今天是2020年3月8日11:40，天迄晴朗，天气很不错，空气很儫，不差；这个nlpcad包，用于方便一键数据增强，犐有效增牆NLP模型的橎化性能、减少波动、抵抗对抗攻击\n",
      "随机字删除>>>>>>\n",
      "这是个实体：58同城；今天是2020年3月8日11:40，天气晴朗，天气很不错，空气很好，不差；这个nlpcad包，用于方便一键数据增强，可有效增强NLP模型的泛化性能、减少波动、抵抗对抗攻击\n",
      "这是个实体：58同城；今天是2020年3月8日11:40，天气晴朗，天气很不错，空气，不差；这个nlpcad包用于方便一键数据增强，可有效增强NLP模型的泛化性能、减少波动、抵抗对抗\n",
      "个实体：58同城；今天是2020年3月8日11:40，天气晴朗，天气很不错空气很好，不差；这个nlpcad包，用于方便一键数据增强，可有效增强NLP模型泛化性能、减少波动、抵抗对抗\n",
      "等价字替换>>>>>>\n",
      "这是个实体：58同城；今天是2020年3月8日11:40，天气晴朗，天气很不错，空气很好，不差；这个nlpcad包，用于方便一键数据增强，可有效增强NLP模型的泛化性能、减少波动、抵抗对抗攻击\n",
      "这是个实体：五捌同城；今天是2〇2零年3月⑧日1壹:四0，天气晴朗，天气很不错，空气很好，不差；这个nlpcad包，用于方便壹键数据增强，可有效增强NLP模型的泛化性能、减少波动、抵抗对抗攻击\n",
      "这是个实体：五8同城；今天是贰020年3月8日11:④〇，天气晴朗，天气很不错，空气很好，不差；这个nlpcad包，用于方便一键数据增强，可有效增强NLP模型的泛化性能、减少波动、抵抗对抗攻击\n",
      "这是个实体：5⑧同城；今天是2〇贰0年3月8日11:4〇，天气晴朗，天气很不错，空气很好，不差；这个nlpcad包，用于方便一键数据增强，可有效增强NLP模型的泛化性能、减少波动、抵抗对抗攻击\n",
      "这是个实体：58同城；今天是20二0年③月⑧日11:4零，天气晴朗，天气很不错，空气很好，不差；这个nlpcad包，用于方便一键数据增强，可有效增强NLP模型的泛化性能、减少波动、抵抗对抗攻击\n",
      "这是个实体：5捌同城；今天是2〇20年3月八日一1:40，天气晴朗，天气很不错，空气很好，不差；这个nlpcad包，用于方便一键数据增强，可有效增强NLP模型的泛化性能、减少波动、抵抗对抗攻击\n",
      "这是个实体：⑤⑧同城；今天是贰020年③月8日①一:肆0，天气晴朗，天气很不错，空气很好，不差；这个nlpcad包，用于方便①键数据增强，可有效增强NLP模型的泛化性能、减少波动、抵抗对抗攻击\n",
      "这是个实体：伍八同城；今天是202零年3月8日1一:40，天气晴朗，天气很不错，空气很好，不差；这个nlpcad包，用于方便一键数据增强，可有效增强NLP模型的泛化性能、减少波动、抵抗对抗攻击\n",
      "这是个实体：⑤八同城；今天是贰零2〇年三月八日11:④〇，天气晴朗，天气很不错，空气很好，不差；这个nlpcad包，用于方便壹键数据增强，可有效增强NLP模型的泛化性能、减少波动、抵抗对抗攻击\n",
      "这是个实体：伍8同城；今天是贰02〇年3月八日一1:④0，天气晴朗，天气很不错，空气很好，不差；这个nlpcad包，用于方便1键数据增强，可有效增强NLP模型的泛化性能、减少波动、抵抗对抗攻击\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from nlpcda import Randomword\n",
    "from nlpcda import Similarword\n",
    "from nlpcda import Homophone\n",
    "from nlpcda import RandomDeleteChar\n",
    "from nlpcda import Ner\n",
    "from nlpcda import CharPositionExchange\n",
    "from nlpcda import baidu_translate\n",
    "from nlpcda import EquivalentChar\n",
    "\n",
    "\n",
    "def test_Randomword(test_str, create_num=3, change_rate=0.3):\n",
    "    '''\n",
    "    随机【（等价）实体】替换，这里是extdata/company.txt ，随机公司实体替换\n",
    "    :param test_str: 替换文本\n",
    "    :param create_num: 增强为多少个\n",
    "    :param change_rate: 文本变化率/最大多少比例会被改变\n",
    "    :return:\n",
    "    '''\n",
    "    smw = Randomword(create_num=create_num, change_rate=change_rate)\n",
    "    return smw.replace(test_str)\n",
    "\n",
    "\n",
    "def test_Similarword(test_str, create_num=3, change_rate=0.3):\n",
    "    '''\n",
    "    随机【同义词】替换\n",
    "    :param test_str: 替换文本\n",
    "    :param create_num: 增强为多少个\n",
    "    :param change_rate: 文本变化率/最大多少比例会被改变\n",
    "    :return:\n",
    "    '''\n",
    "    smw = Similarword(create_num=create_num, change_rate=change_rate)\n",
    "    return smw.replace(test_str)\n",
    "\n",
    "\n",
    "def test_Homophone(test_str, create_num=3, change_rate=0.1):\n",
    "    '''\n",
    "    随机【同意/同音字】替换\n",
    "    :param test_str: 替换文本\n",
    "    :param create_num: 增强为多少个\n",
    "    :param change_rate: 文本变化率/最大多少比例会被改变\n",
    "    :return:\n",
    "    '''\n",
    "    hoe = Homophone(create_num=create_num, change_rate=change_rate)\n",
    "    return hoe.replace(test_str)\n",
    "\n",
    "\n",
    "def test_RandomDeleteChar(test_str, create_num=3, change_rate=0.1):\n",
    "    smw = RandomDeleteChar(create_num=create_num, change_rate=change_rate)\n",
    "    return smw.replace(test_str)\n",
    "\n",
    "\n",
    "\n",
    "def test_ner():\n",
    "    ner = Ner(ner_dir_name='../write',\n",
    "              ignore_tag_list=['O', 'T'],\n",
    "              data_augument_tag_list=['Cause', 'Effect'],\n",
    "              augument_size=3, seed=0)\n",
    "    data_sentence_arrs, data_label_arrs = ner.augment('../write/1.txt')\n",
    "    print(data_sentence_arrs, data_label_arrs)\n",
    "\n",
    "\n",
    "def test_CharPositionExchange(test_str, create_num=10, change_rate=0.5):\n",
    "    smw = CharPositionExchange(create_num=create_num, change_rate=change_rate)\n",
    "    return smw.replace(test_str)\n",
    "\n",
    "\n",
    "def test_baidu_translate():\n",
    "    a = 'Free translation for each platform'\n",
    "    s = baidu_translate(a, appid='xxx', secretKey='xxx')\n",
    "    print(s)\n",
    "\n",
    "\n",
    "def test_EquivalentChar(test_str, create_num=10, change_rate=0.5):\n",
    "    s = EquivalentChar(create_num=create_num, change_rate=change_rate)\n",
    "    return s.replace(test_str)\n",
    "\n",
    "\n",
    "def test():\n",
    "    ts = '''这是个实体：58同城；今天是2020年3月8日11:40，天气晴朗，天气很不错，空气很好，不差；这个nlpcad包，用于方便一键数据增强，可有效增强NLP模型的泛化性能、减少波动、抵抗对抗攻击'''\n",
    "    rs1 = test_Randomword(ts)\n",
    "    rs2 = test_Similarword(ts)\n",
    "    rs3 = test_Homophone(ts)\n",
    "    rs4 = test_RandomDeleteChar(ts)\n",
    "    rs5 = test_EquivalentChar(ts)\n",
    "    print('随机实体替换>>>>>>')\n",
    "    for s in rs1:\n",
    "        print(s)\n",
    "    print('随机近义词替换>>>>>>')\n",
    "    for s in rs2:\n",
    "        print(s)\n",
    "    print('随机近义字替换>>>>>>')\n",
    "    for s in rs3:\n",
    "        print(s)\n",
    "\n",
    "    print('随机字删除>>>>>>')\n",
    "    for s in rs4:\n",
    "        print(s)\n",
    "    print('等价字替换>>>>>>')\n",
    "    for s in rs5:\n",
    "        print(s)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "#     # ts = '''今天是2020年3月8日11:40，天气晴朗，天气很不错。'''\n",
    "#     # rs = EquivalentChar(create_num=3, change_rate=0.5)\n",
    "#     # res = rs.replace(ts)\n",
    "#     # print('等价字替换>>>>>>')\n",
    "#     # for s in res:\n",
    "#     #     print(s)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d47d58cb-210d-4a5c-a339-2e398687801d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# config = {\n",
    "#         'model_path': '/root/autodl-tmp/src/chinese_simbert_L-12_H-768_A-12',\n",
    "#         'CUDA_VISIBLE_DEVICES': '0',\n",
    "#         'max_len': 32,\n",
    "#         'seed': 1\n",
    "# }\n",
    "# simbert = Simbert(config=config)\n",
    "# sent = '把我的一个亿存银行安全吗'\n",
    "# synonyms = simbert.replace(sent=sent, create_num=5)\n",
    "# print(synonyms)\n",
    "# '''\n",
    "# [('我的一个亿，存银行，安全吗', 0.9871675372123718), \n",
    "# ('把一个亿存到银行里安全吗', 0.9352194666862488), \n",
    "# ('一个亿存银行安全吗', 0.9330801367759705), \n",
    "# ('一个亿的存款存银行安全吗', 0.92387855052948),\n",
    "#  ('我的一千万存到银行安不安全', 0.9014463424682617)]\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "274da621-2a58-46f8-9011-2370ca309fce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.utils import io\n",
    "\n",
    "# with io.capture_output() as captured:\n",
    "#     load('/root/miniconda3/lib/python3.8/site-packages/nlpcda/data/company.txt') \n",
    "\n",
    "# print(captured.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8760db-5f90-4750-b3ae-3d05c0278d78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "augment_rate2 = {0: 5331,1: 2477,2: 5072,3: 6663,4: 544 * 2,5: 1734,6: 2046,7: 908*2,8: 267*3,9: 639*2}\n",
    "def augment_nlpcda(ts):\n",
    "    smw1 = Randomword(create_num=1, change_rate=0.3)\n",
    "    smw2 = Similarword(create_num=1, change_rate=0.3)\n",
    "    hoe = Homophone(create_num=1, change_rate=0.1)\n",
    "    smw3 = RandomDeleteChar(create_num=1, change_rate=0.1)\n",
    "    s = EquivalentChar(create_num=2, change_rate=0.5)\n",
    "    # return s.replace(ts)[0]\n",
    "    res = s.replace(ts)\n",
    "    return [smw1.replace(smw2.replace(hoe.replace(smw3.replace(i)[0])[0])[0])[0] for i in res]\n",
    "\n",
    "def augment_minority_augmentSample(train_df, augment_rate):\n",
    "    oversample = RandomOverSampler(sampling_strategy = augment_rate)\n",
    "    X_over, y_over = oversample.fit_resample(train_df.drop('tag', axis=1), train_df['tag'])\n",
    "    X_over['tag'] = y_over\n",
    "    return X_over\n",
    "train_aug = train_df[train_df['tag'].isin([4,7,8,9])]\n",
    "with io.capture_output() as captured:\n",
    "    train_aug1 = train_aug.copy()\n",
    "    train_aug2 = train_aug.copy()\n",
    "    tmp = train_aug['content'].apply(lambda x: augment_nlpcda(x)).tolist()\n",
    "    res1 = [i[0] for i in tmp]\n",
    "    res2 = [i[1] for i in tmp]\n",
    "    train_aug1['content'] = res1\n",
    "    train_aug2['content'] = res2\n",
    "    train_aug_final = pd.concat([train_df, train_aug1, train_aug2],axis=1).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5c3813-f728-4fdf-a4b5-d53eba2c9525",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_aug_final.to_json(datPath, 'new_train_nlpcda.jason')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86824da4-ff91-4411-87ef-12ea9dfdecf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_model_v4 = model_train(new_model, new_train_dataloader, val_dataloader)\n",
    "test_accuracy, test_cat_acc, test_pred = evaluate(new_model_v4, test_dataloader)\n",
    "print(\"Test Accuracy:\", test_accuracy, \"Test Cat Accuracy:\", test_cat_acc)\n",
    "new_model_v4 = save_model(new_model_v4, wtPath, model_name = 'BertRetrainNewAllDatRemErrTrainNlpcdaAug_v4.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb7b06b-e69e-452b-b9df-bee69d8f254f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
