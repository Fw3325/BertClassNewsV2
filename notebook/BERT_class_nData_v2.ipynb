{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729c6f77-211f-4b74-a2ca-c835c117c845",
   "metadata": {
    "tags": [],
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "# from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "datPath = '/root/autodl-tmp/BertClassNews/data/'\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "b3b36232f46ebebc"
  },
  {
   "cell_type": "markdown",
   "id": "27169d4d-438a-4c1b-bc52-ab98dfe6bc92",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe38de7e-194a-4a9f-81de-fab93deb279f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "datPath = '/root/autodl-tmp/BertClassNews/data/'\n",
    "wtPath = '/root/autodl-tmp/BertClassNews/wt/'\n",
    "\n",
    "train_df = pd.read_json(datPath + 'train_cls.json')\n",
    "val_df = pd.read_json(datPath + 'val_cls.json')\n",
    "test_df = pd.read_json(datPath + 'test_cls.json')\n",
    "\n",
    "# train_df = pd.read_json(datPath + 'smpl_train.json')\n",
    "# val_df = pd.read_json(datPath + 'smpl_val.json')\n",
    "# test_df = pd.read_json(datPath + 'smpl_test.json')\n",
    "\n",
    "lblEncode = {j:i for i,j in zip(range(len(train_df['tag'].unique())), train_df['tag'].unique())}\n",
    "train_df['tag'] = train_df['tag'].map(lblEncode)\n",
    "val_df['tag'] = val_df['tag'].map(lblEncode)\n",
    "test_df['tag'] = test_df['tag'].map(lblEncode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4a73b7f-98a3-4b7a-8a5e-46f29db5b36b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e252896-ddf9-43c4-8b57-e28dd6a1fbfa",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5268139-7308-4a78-a117-135da35b4396",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch transformers\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "# lblEncode = {j:i for i,j in zip(range(len(train_df['tag'].unique())), train_df['tag'].unique())}\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-chinese',num_labels=len(lblEncode))\n",
    "# inputs = tokenizer(text, return_tensors='pt', padding=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "155f749b-7882-4b56-867f-b5f2a195a067",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reverse_lblEncode = {lblEncode[i]:i for i in lblEncode}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "500e6b86-3284-4cca-8277-e7bfe8e4d314",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'本地旅游': 0,\n",
       " '通报查处': 1,\n",
       " '基建民生': 2,\n",
       " '社会热点': 3,\n",
       " '暖新闻': 4,\n",
       " '人事任免': 5,\n",
       " '政策类型': 6,\n",
       " '产业金融': 7,\n",
       " '人文历史': 8,\n",
       " '数据排名': 9}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lblEncode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ab9b68-9ca4-4271-8f99-09c44541a63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cdf6a34-a090-473b-8e03-b825766d6b42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import sklearn.model_selection\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8da66e1e-602c-4e13-870a-03657c0eee14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reverse_lblEncode = {lblEncode[i]:i for i in lblEncode}\n",
    "\n",
    "class BERTDataset(Dataset):\n",
    "\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.inputs.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_texts, val_texts, test_texts = train_df['content'].tolist(), val_df['content'].tolist(), test_df['content'].tolist()\n",
    "train_labels, val_labels, test_labels = train_df['tag'].tolist(), val_df['tag'].tolist(), test_df['tag'].tolist()\n",
    "\n",
    "# train_texts, val_texts, test_texts = train_df['content'].iloc[:100].tolist(), val_df['content'].iloc[:100].tolist(), test_df['content'].iloc[:100].tolist()\n",
    "# train_labels, val_labels, test_labels = train_df['tag'].iloc[:100].tolist(), val_df['tag'].iloc[:100].tolist(), test_df['tag'].iloc[:100].tolist()\n",
    "def generateDataloader(df, lbl, BATCH_SIZE = 16):\n",
    "    inputs = tokenizer(df, padding=True, truncation=True,return_tensors='pt')\n",
    "    dataset = BERTDataset(inputs, lbl)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "    return dataloader\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    final_pred = []\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    pred = {i:[] for i in range(len(lblEncode))}\n",
    "    # pred = {i:[] for i in [0,2,3]}\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch in dataloader:\n",
    "            inputs = torch.stack([t for t in batch['input_ids']]).to(device)\n",
    "            labels = torch.tensor(batch['labels']).to(device)\n",
    "            attention_mask =torch.stack([t for t in batch['attention_mask']]).to(device)\n",
    "            outputs = model(input_ids=inputs, attention_mask=attention_mask,\n",
    "                      labels=labels)\n",
    "            predictions = outputs[1].argmax(dim=1)\n",
    "            for i in range(len(predictions)):\n",
    "                predRes = predictions[i].item()\n",
    "                pred[predRes].append((predictions[i] == labels[i]).item())\n",
    "                final_pred.append((reverse_lblEncode[predRes], reverse_lblEncode[labels[i].item()]))\n",
    "                # final_pred.append((predRes, labels[i].item()))\n",
    "\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.shape[0]\n",
    "      \n",
    "    accuracy = correct/total\n",
    "    res = {reverse_lblEncode[i]:sum(pred[i])/len(pred[i]) if len(pred[i]) > 0 else np.nan for i in pred }\n",
    "  # res = {i:sum(pred[i])/len(pred[i]) if len(pred[i]) > 0 else np.nan for i in pred }\n",
    "\n",
    "    return accuracy, res, final_pred\n",
    "\n",
    "def model_train(model, train_dataloader, val_dataloader, iter = 3):\n",
    "    for epoch in range(3):\n",
    "      start = time.time()\n",
    "      losses = 0\n",
    "      model.train()\n",
    "      idx = 0\n",
    "      for batch in train_dataloader:\n",
    "        inputs, token_type_ids, attention_mask, labels = batch['input_ids'], batch['token_type_ids'], batch['attention_mask'], batch['labels']\n",
    "\n",
    "        input_ids = torch.stack([t for t in batch['input_ids']]).to(device)\n",
    "\n",
    "        attention_mask =torch.stack([t for t in batch['attention_mask']]).to(device)\n",
    "        labels = torch.tensor(batch['labels']).to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask,labels=labels)\n",
    "        loss = criterion(outputs['logits'],labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        losses += loss.item()\n",
    "        acc = (outputs['logits'].argmax(1) == labels).float().mean()\n",
    "        idx += 1\n",
    "        curr = time.time()\n",
    "        timeConsume = curr - start\n",
    "        if idx % 300 == 0:\n",
    "            print (f'Epoch {epoch+1}, batch {idx}, so far takes {timeConsume}')\n",
    "          # los  =outputs['loss']\n",
    "            print (f'Loss: {losses}')\n",
    "\n",
    "      print (acc)\n",
    "      val_acc, val_cat_acc, val_final_pred= evaluate(model, val_dataloader)\n",
    "      print ('val acc:', val_acc)\n",
    "        # if idx % 10 == 0:\n",
    "        #     logging.info(f\"Epoch: {epoch}, Batch[{idx}/{len(train_iter)}], \"\n",
    "        #                   f\"Train loss :{loss.item():.3f}, Train acc: {acc:.3f}\")\n",
    "\n",
    "      print(f'Epoch {epoch+1} complete, so far takes {timeConsume}')\n",
    "    return model\n",
    "\n",
    "\n",
    "def save_model(model, datPath, model_name = 'BertOrigmodelAllDat_v0.pt'):\n",
    "    # datPath = '/root/wt/'\n",
    "    # model_save_path  = datPath + 'BertOrigmodelAllDat_v0.pt'\n",
    "    model_save_path  = datPath + model_name\n",
    "    if os.path.exists(model_save_path):\n",
    "        loaded_paras = torch.load(model_save_path)\n",
    "        model.load_state_dict(loaded_paras)\n",
    "        logging.info(\"## 成功载入已有模型，进行追加训练......\")\n",
    "    else:\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47009d29-ed2a-4640-afa1-e69db910b855",
   "metadata": {},
   "source": [
    "## Prepare for new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a590819-4ec6-445d-8eea-9fe2b5264c9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader = generateDataloader(train_df['content'].tolist(), train_df['tag'].tolist()) \n",
    "val_dataloader = generateDataloader(val_df['content'].tolist(), val_df['tag'].tolist()) \n",
    "test_dataloader = generateDataloader(test_df['content'].tolist(), test_df['tag'].tolist()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6614f825-e5fb-473e-9977-c7b8c02f4509",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "def remove_part_fPred_train(train_df, train_pred):\n",
    "    cond_acc = []\n",
    "    for i,j in train_pred:\n",
    "        cond_acc.append((i==j) |((i!=j)&(j not in ['本地旅游','基建民生','社会热点'])))\n",
    "    # cond_acc = pd.Series(cond_acc)\n",
    "    new_train = train_df[cond_acc]\n",
    "    new_train = new_train.reset_index(drop = True)\n",
    "    return new_train\n",
    "\n",
    "def augment_minority_randomSample(train_df, augment_rate):\n",
    "    oversample = RandomOverSampler(sampling_strategy = augment_rate)\n",
    "    X_over, y_over = oversample.fit_resample(train_df.drop('tag', axis=1), train_df['tag'])\n",
    "    X_over['tag'] = y_over\n",
    "    return X_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7401b80d-c65a-43de-9169-ffe94859eff4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_model = save_model(model, wtPath, model_name = 'BertRetrainNewAllDat_v1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9faeaa0-e7d4-4468-bec4-36889ea171f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8826125330979699 Test Cat Accuracy: {'本地旅游': 0.9641255605381166, '通报查处': 0.8914893617021277, '基建民生': 0.8492723492723493, '社会热点': 0.9102112676056338, '暖新闻': 0.6333333333333333, '人事任免': 0.9675090252707581, '政策类型': 0.8978328173374613, '产业金融': 0.7604790419161677, '人文历史': 0.423728813559322, '数据排名': 0.6666666666666666}\n"
     ]
    }
   ],
   "source": [
    "test_accuracy, test_cat_acc, test_pred = evaluate(new_model, test_dataloader)\n",
    "print(\"Test Accuracy:\", test_accuracy, \"Test Cat Accuracy:\", test_cat_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98fb63b-16e7-49d3-88c6-5aa984fa33b4",
   "metadata": {},
   "source": [
    "# Train based on augmented "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "055a21e9-ee8a-410d-80be-5f9fe7610f5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simbert不能正常使用，除非你安装：bert4keras、tensorflow ，为了安装快捷，没有默认安装.... No module named 'bert4keras'\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from nlpcda import Randomword\n",
    "from nlpcda import Similarword\n",
    "from nlpcda import Homophone\n",
    "from nlpcda import RandomDeleteChar\n",
    "from nlpcda import Ner\n",
    "from nlpcda import CharPositionExchange\n",
    "from nlpcda import baidu_translate\n",
    "from nlpcda import EquivalentChar\n",
    "\n",
    "\n",
    "def test_Randomword(test_str, create_num=3, change_rate=0.3):\n",
    "    '''\n",
    "    随机【（等价）实体】替换，这里是extdata/company.txt ，随机公司实体替换\n",
    "    :param test_str: 替换文本\n",
    "    :param create_num: 增强为多少个\n",
    "    :param change_rate: 文本变化率/最大多少比例会被改变\n",
    "    :return:\n",
    "    '''\n",
    "    smw = Randomword(create_num=create_num, change_rate=change_rate)\n",
    "    return smw.replace(test_str)\n",
    "\n",
    "\n",
    "def test_Similarword(test_str, create_num=3, change_rate=0.3):\n",
    "    '''\n",
    "    随机【同义词】替换\n",
    "    :param test_str: 替换文本\n",
    "    :param create_num: 增强为多少个\n",
    "    :param change_rate: 文本变化率/最大多少比例会被改变\n",
    "    :return:\n",
    "    '''\n",
    "    smw = Similarword(create_num=create_num, change_rate=change_rate)\n",
    "    return smw.replace(test_str)\n",
    "\n",
    "\n",
    "def test_Homophone(test_str, create_num=3, change_rate=0.1):\n",
    "    '''\n",
    "    随机【同意/同音字】替换\n",
    "    :param test_str: 替换文本\n",
    "    :param create_num: 增强为多少个\n",
    "    :param change_rate: 文本变化率/最大多少比例会被改变\n",
    "    :return:\n",
    "    '''\n",
    "    hoe = Homophone(create_num=create_num, change_rate=change_rate)\n",
    "    return hoe.replace(test_str)\n",
    "\n",
    "\n",
    "def test_RandomDeleteChar(test_str, create_num=3, change_rate=0.1):\n",
    "    smw = RandomDeleteChar(create_num=create_num, change_rate=change_rate)\n",
    "    return smw.replace(test_str)\n",
    "\n",
    "\n",
    "\n",
    "def test_ner():\n",
    "    ner = Ner(ner_dir_name='../write',\n",
    "              ignore_tag_list=['O', 'T'],\n",
    "              data_augument_tag_list=['Cause', 'Effect'],\n",
    "              augument_size=3, seed=0)\n",
    "    data_sentence_arrs, data_label_arrs = ner.augment('../write/1.txt')\n",
    "    print(data_sentence_arrs, data_label_arrs)\n",
    "\n",
    "\n",
    "def test_CharPositionExchange(test_str, create_num=10, change_rate=0.5):\n",
    "    smw = CharPositionExchange(create_num=create_num, change_rate=change_rate)\n",
    "    return smw.replace(test_str)\n",
    "\n",
    "\n",
    "def test_baidu_translate():\n",
    "    a = 'Free translation for each platform'\n",
    "    s = baidu_translate(a, appid='xxx', secretKey='xxx')\n",
    "    print(s)\n",
    "\n",
    "\n",
    "def test_EquivalentChar(test_str, create_num=10, change_rate=0.5):\n",
    "    s = EquivalentChar(create_num=create_num, change_rate=change_rate)\n",
    "    return s.replace(test_str)\n",
    "\n",
    "\n",
    "def test():\n",
    "    ts = '''这是个实体：58同城；今天是2020年3月8日11:40，天气晴朗，天气很不错，空气很好，不差；这个nlpcad包，用于方便一键数据增强，可有效增强NLP模型的泛化性能、减少波动、抵抗对抗攻击'''\n",
    "    rs1 = test_Randomword(ts)\n",
    "    rs2 = test_Similarword(ts)\n",
    "    rs3 = test_Homophone(ts)\n",
    "    rs4 = test_RandomDeleteChar(ts)\n",
    "    rs5 = test_EquivalentChar(ts)\n",
    "    print('随机实体替换>>>>>>')\n",
    "    for s in rs1:\n",
    "        print(s)\n",
    "    print('随机近义词替换>>>>>>')\n",
    "    for s in rs2:\n",
    "        print(s)\n",
    "    print('随机近义字替换>>>>>>')\n",
    "    for s in rs3:\n",
    "        print(s)\n",
    "\n",
    "    print('随机字删除>>>>>>')\n",
    "    for s in rs4:\n",
    "        print(s)\n",
    "    print('等价字替换>>>>>>')\n",
    "    for s in rs5:\n",
    "        print(s)\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     # ts = '''今天是2020年3月8日11:40，天气晴朗，天气很不错。'''\n",
    "#     # rs = EquivalentChar(create_num=3, change_rate=0.5)\n",
    "#     # res = rs.replace(ts)\n",
    "#     # print('等价字替换>>>>>>')\n",
    "#     # for s in res:\n",
    "#     #     print(s)\n",
    "    # test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc134cca-504e-41e4-9828-395474561e55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.utils import io\n",
    "# {'本地旅游': 0,\n",
    "#  '通报查处': 1,\n",
    "#  '基建民生': 2,\n",
    "#  '社会热点': 3,\n",
    "#  '暖新闻': 4,\n",
    "#  '人事任免': 5,\n",
    "#  '政策类型': 6,\n",
    "#  '产业金融': 7,\n",
    "#  '人文历史': 8,\n",
    "#  '数据排名': 9}\n",
    "augment_rate2 = {0: 5331,1: 2477,2: 5072,3: 6663,4: 544 * 2,5: 1734,6: 2046,7: 908*2,8: 267*3,9: 639*2}\n",
    "# new_train = remove_part_fPred_train(train_df, train_pred)\n",
    "new_train = pd.read_json(datPath + 'new_train_cls.json')\n",
    "# new_train = augment_minority_randomSample(new_train, augment_rate2)\n",
    "# new_train_dataloader = generateDataloader(new_train['content'].tolist(), new_train['tag'].tolist()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2506ea89-0e49-4c4c-922f-4fffdfa91629",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "59782ec7-d3d1-45c2-a671-9e9147cd84d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load :/root/miniconda3/lib/python3.8/site-packages/nlpcda/data/company.txt done\n",
      "load :/root/miniconda3/lib/python3.8/site-packages/nlpcda/data/同义词.txt done\n",
      "load :/root/miniconda3/lib/python3.8/site-packages/nlpcda/data/同音意字.txt done\n",
      "load :/root/miniconda3/lib/python3.8/site-packages/nlpcda/data/等价字.txt done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'雨天黔西南男子骑摩托被困激流中，私家车挡住激流，众人冒雨扶车。。'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "augment_nlpcda(tmp[17][0], equalNum = 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5ae564cf-3b09-4126-bd5d-32ee7bcd9f45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# res1 = [i[0] for i in tmp]\n",
    "\n",
    "# [j[1] for j in tmp]\n",
    "# len(res1), len(tmp), train_aug.shape\n",
    "\n",
    "res1, res2 = [], []\n",
    "with io.capture_output() as captured:\n",
    "    for i in tmp:\n",
    "        if len(i) == 2:\n",
    "            res1.append(i[0])\n",
    "            res2.append(i[1])\n",
    "        elif len(i) == 1:\n",
    "            res1.append(i[0])\n",
    "            res2.append(augment_nlpcda(i[0], equalNum = 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "591ce13a-9869-4947-88a5-ca4cfa8d67ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "augment_rate2 = {0: 5331,1: 2477,2: 5072,3: 6663,4: 544 * 2,5: 1734,6: 2046,7: 908*2,8: 267*3,9: 639*2}\n",
    "def augment_nlpcda(ts, equalNum = 2):\n",
    "    smw1 = Randomword(create_num=1, change_rate=0.3)\n",
    "    smw2 = Similarword(create_num=1, change_rate=0.3)\n",
    "    hoe = Homophone(create_num=1, change_rate=0.1)\n",
    "    smw3 = RandomDeleteChar(create_num=1, change_rate=0.1)\n",
    "    s = EquivalentChar(create_num=equalNum, change_rate=0.5)\n",
    "    # return s.replace(ts)[0]\n",
    "    res = s.replace(ts)\n",
    "    return [smw1.replace(smw2.replace(hoe.replace(smw3.replace(i)[0])[0])[0])[0] for i in res]\n",
    "\n",
    "def augment_minority_augmentSample(train_df, augment_rate):\n",
    "    oversample = RandomOverSampler(sampling_strategy = augment_rate)\n",
    "    X_over, y_over = oversample.fit_resample(train_df.drop('tag', axis=1), train_df['tag'])\n",
    "    X_over['tag'] = y_over\n",
    "    return X_over\n",
    "train_aug = train_df[train_df['tag'].isin([4,7,8,9])]\n",
    "with io.capture_output() as captured:\n",
    "    train_aug1 = train_aug.copy()\n",
    "    train_aug2 = train_aug.copy()\n",
    "    tmp = train_aug['content'].apply(lambda x: augment_nlpcda(x)).tolist()\n",
    "res1, res2 = [], []\n",
    "with io.capture_output() as captured:\n",
    "    for i in tmp:\n",
    "        if len(i) == 2:\n",
    "            res1.append(i[0])\n",
    "            res2.append(i[1])\n",
    "        elif len(i) == 1:\n",
    "            res1.append(i[0])\n",
    "            res2.append(augment_nlpcda(i[0], equalNum = 1)[0])\n",
    "train_aug1['content'] = res1\n",
    "train_aug2['content'] = res2\n",
    "train_aug_final = pd.concat([train_df, train_aug1, train_aug2],axis=0).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3f0fa7c5-76d4-421a-9271-d6303a39d583",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_aug_final.to_json(datPath+'new_train_nlpcda.jason')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e80dea2f-c928-4a0d-af46-96e965193e26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_aug_final_dataloader = generateDataloader(train_aug_final['content'].tolist(), train_aug_final['tag'].tolist()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2158bd52-ef85-4bec-9465-666228cf8675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, batch 300, so far takes 102.95491433143616\n",
      "Loss: 49.14178957277909\n",
      "Epoch 1, batch 600, so far takes 205.76029801368713\n",
      "Loss: 95.65615228097886\n",
      "Epoch 1, batch 900, so far takes 308.6040370464325\n",
      "Loss: 144.49581548944116\n",
      "Epoch 1, batch 1200, so far takes 411.64946126937866\n",
      "Loss: 195.18263155594468\n",
      "Epoch 1, batch 1500, so far takes 514.8050725460052\n",
      "Loss: 241.99835541006178\n",
      "Epoch 1, batch 1800, so far takes 617.715742111206\n",
      "Loss: 298.6685672462918\n",
      "tensor(1., device='cuda:0')\n",
      "val acc: 0.8852083651350939\n",
      "Epoch 1 complete, so far takes 663.1937487125397\n",
      "Epoch 2, batch 300, so far takes 102.77483129501343\n",
      "Loss: 41.76007594168186\n",
      "Epoch 2, batch 600, so far takes 205.87410306930542\n",
      "Loss: 76.46334711485542\n",
      "Epoch 2, batch 900, so far takes 308.83878898620605\n",
      "Loss: 114.0493159990292\n",
      "Epoch 2, batch 1200, so far takes 411.73110246658325\n",
      "Loss: 150.53454364836216\n",
      "Epoch 2, batch 1500, so far takes 514.8581447601318\n",
      "Loss: 189.45315896021202\n",
      "Epoch 2, batch 1800, so far takes 617.9172775745392\n",
      "Loss: 230.1854638911318\n",
      "tensor(1., device='cuda:0')\n",
      "val acc: 0.8936040299190964\n",
      "Epoch 2 complete, so far takes 663.4190793037415\n",
      "Epoch 3, batch 300, so far takes 103.01299595832825\n",
      "Loss: 41.788841226138175\n",
      "Epoch 3, batch 600, so far takes 206.07651710510254\n",
      "Loss: 72.77736436016858\n",
      "Epoch 3, batch 900, so far takes 309.0433793067932\n",
      "Loss: 109.27885162946768\n",
      "Epoch 3, batch 1200, so far takes 412.29371333122253\n",
      "Loss: 142.2523127135355\n",
      "Epoch 3, batch 1500, so far takes 515.4349138736725\n",
      "Loss: 178.50197999901138\n",
      "Epoch 3, batch 1800, so far takes 618.3865232467651\n",
      "Loss: 212.62796739139594\n",
      "tensor(1., device='cuda:0')\n",
      "val acc: 0.8926881392153869\n",
      "Epoch 3 complete, so far takes 663.8600580692291\n",
      "Test Accuracy: 0.8815092674315975 Test Cat Accuracy: {'本地旅游': 0.9236326109391125, '通报查处': 0.9220489977728286, '基建民生': 0.8602620087336245, '社会热点': 0.9215509467989179, '暖新闻': 0.6153846153846154, '人事任免': 0.9708029197080292, '政策类型': 0.8556149732620321, '产业金融': 0.7258064516129032, '人文历史': 0.48717948717948717, '数据排名': 0.6511627906976745}\n"
     ]
    }
   ],
   "source": [
    "new_model_v4 = model_train(new_model, train_aug_final_dataloader, val_dataloader)\n",
    "test_accuracy, test_cat_acc, test_pred = evaluate(new_model_v4, test_dataloader)\n",
    "print(\"Test Accuracy:\", test_accuracy, \"Test Cat Accuracy:\", test_cat_acc)\n",
    "new_model_v4 = save_model(new_model_v4, wtPath, model_name = 'BertRetrainNewAllDatRemErrTrainNlpcdaAug_v4.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "59d27b83-e1c6-4f5a-a6f1-b0d24cb58e34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, batch 300, so far takes 102.95349550247192\n",
      "Loss: 35.51844134321436\n",
      "Epoch 1, batch 600, so far takes 205.94252490997314\n",
      "Loss: 76.25066650775261\n",
      "Epoch 1, batch 900, so far takes 308.9322326183319\n",
      "Loss: 111.10925501096062\n",
      "Epoch 1, batch 1200, so far takes 411.9672613143921\n",
      "Loss: 145.50120523478836\n",
      "Epoch 1, batch 1500, so far takes 514.8288156986237\n",
      "Loss: 175.0374473250704\n",
      "Epoch 1, batch 1800, so far takes 617.9021968841553\n",
      "Loss: 207.4173150168499\n",
      "tensor(1., device='cuda:0')\n",
      "val acc: 0.8871927949931309\n",
      "Epoch 1 complete, so far takes 663.3985121250153\n",
      "Epoch 2, batch 300, so far takes 103.1886100769043\n",
      "Loss: 29.015644358936697\n",
      "tensor(1., device='cuda:0')\n",
      "val acc: 0.8878033887956037\n",
      "Epoch 2 complete, so far takes 664.2198729515076\n",
      "Epoch 3, batch 300, so far takes 103.32352805137634\n",
      "Loss: 32.25105999573134\n",
      "Epoch 3, batch 600, so far takes 206.35923600196838\n",
      "Loss: 61.50817705481313\n",
      "Epoch 3, batch 900, so far takes 309.558434009552\n",
      "Loss: 89.56576520064846\n",
      "Epoch 3, batch 1200, so far takes 412.8639187812805\n",
      "Loss: 111.32194918044843\n",
      "Epoch 3, batch 1500, so far takes 515.9079670906067\n",
      "Loss: 140.9389998106053\n",
      "Epoch 3, batch 1800, so far takes 618.8607714176178\n",
      "Loss: 169.53882570995484\n",
      "tensor(1., device='cuda:0')\n",
      "val acc: 0.8925354907647688\n",
      "Epoch 3 complete, so far takes 664.4229865074158\n",
      "Test Accuracy: 0.8859223300970874 Test Cat Accuracy: {'本地旅游': 0.9626373626373627, '通报查处': 0.9249448123620309, '基建民生': 0.8807339449541285, '社会热点': 0.8790849673202614, '暖新闻': 0.6371681415929203, '人事任免': 0.96415770609319, '政策类型': 0.9005847953216374, '产业金融': 0.7607361963190185, '人文历史': 0.38095238095238093, '数据排名': 0.6991150442477876}\n"
     ]
    }
   ],
   "source": [
    "new_model_v5 = model_train(new_model_v4, train_aug_final_dataloader, val_dataloader)\n",
    "test_accuracy, test_cat_acc, test_pred = evaluate(new_model_v5, test_dataloader)\n",
    "print(\"Test Accuracy:\", test_accuracy, \"Test Cat Accuracy:\", test_cat_acc)\n",
    "new_model_v5 = save_model(new_model_v5, wtPath, model_name = 'BertRetrainNewAllDatRemErrTrainNlpcdaAug_v5.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508eeca9-a82a-4fa0-9d39-c9efee5a0d32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
