{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ff415ba-21a5-4b9a-b25b-0cf1c1fb05d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "datPath = '/root/autodl-tmp/BertClassNews/data/'\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c7ccfab-cd8b-438b-aa99-576f6120fa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Trpath = datPath + 'train_cls-sample.txt'\n",
    "Testpath = datPath + 'dev_cls-sample.txt'\n",
    "\n",
    "# f = open(pathTr, 'r')\n",
    "# train_val = f.read()\n",
    "\n",
    "def read_process_dat(path):\n",
    "    try:\n",
    "        with open(path) as f:\n",
    "            text = f.read()\n",
    "    except UnicodeDecodeError:        \n",
    "        with open(path, errors='ignore') as f:\n",
    "            text = f.read()\n",
    "    pattern = r'[\\t\\n]'\n",
    "    result = re.split(pattern, text)\n",
    "    contentIndx = 0\n",
    "    lblIndx = 1\n",
    "    n = len(result) - 1\n",
    "    content, label = [], []\n",
    "    while lblIndx < n:\n",
    "        content.append(result[contentIndx])\n",
    "        label.append(result[lblIndx])\n",
    "        contentIndx += 2\n",
    "        lblIndx += 2\n",
    "    df = pd.DataFrame()\n",
    "    df['tag'] = label\n",
    "    df['content'] = content\n",
    "    df['len'] = df['content'].apply(lambda x: sum([i.isalpha() for i in x]))\n",
    "    df = df[df['len']>27]\n",
    "    return df\n",
    "\n",
    "train_val_df = read_process_dat(Trpath)\n",
    "test_df = read_process_dat(Testpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84b13c7d-4fd3-416a-ba24-09e7fe39615c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32751, 3),\n",
       " (4532, 3),\n",
       " Counter({'政策类型': 2557,\n",
       "          '基建民生': 6398,\n",
       "          '社会热点': 8704,\n",
       "          '人事任免': 2167,\n",
       "          '产业金融': 1135,\n",
       "          '本地旅游': 6880,\n",
       "          '通报查处': 3097,\n",
       "          '数据排名': 799,\n",
       "          '暖新闻': 680,\n",
       "          '人文历史': 334}),\n",
       " Counter({'数据排名': 111,\n",
       "          '社会热点': 1205,\n",
       "          '基建民生': 889,\n",
       "          '本地旅游': 936,\n",
       "          '政策类型': 370,\n",
       "          '人事任免': 280,\n",
       "          '通报查处': 436,\n",
       "          '暖新闻': 95,\n",
       "          '产业金融': 171,\n",
       "          '人文历史': 39}))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val_df.shape, test_df.shape, Counter(train_val_df['tag'].tolist()), Counter(test_df['tag'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "efe1e37f-02e7-4d92-9267-47227903a1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train valid split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.2, stratify=train_val_df['tag'], random_state=42)\n",
    "train_df.to_json(datPath + 'train_cls.json')\n",
    "val_df.to_json(datPath + 'val_cls.json')\n",
    "test_df.to_json(datPath + 'test_cls.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2bb538-212d-4bf0-8c17-99914c7e12cf",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bd1249f-e8b0-484d-9f35-b19e774b504b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "datPath = '/root/autodl-tmp/BertClassNews/data/'\n",
    "train_df = pd.read_json(datPath + 'train_cls.json')\n",
    "val_df = pd.read_json(datPath + 'val_cls.json')\n",
    "test_df = pd.read_json(datPath + 'test_cls.json')\n",
    "\n",
    "# train_df = pd.read_json(datPath + 'smpl_train.json')\n",
    "# val_df = pd.read_json(datPath + 'smpl_val.json')\n",
    "# test_df = pd.read_json(datPath + 'smpl_test.json')\n",
    "\n",
    "lblEncode = {j:i for i,j in zip(range(len(train_df['tag'].unique())), train_df['tag'].unique())}\n",
    "train_df['tag'] = train_df['tag'].map(lblEncode)\n",
    "val_df['tag'] = val_df['tag'].map(lblEncode)\n",
    "test_df['tag'] = test_df['tag'].map(lblEncode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b086a0c-3af0-4dc8-b4e5-fad0e87394b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a38926-edfe-427b-9605-bfa19370ef85",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "947775e2-6166-4cd8-9f8c-fd91e416bfb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: torch in /root/miniconda3/lib/python3.8/site-packages (2.0.0+cu118)\n",
      "Requirement already satisfied: transformers in /root/miniconda3/lib/python3.8/site-packages (4.33.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /root/miniconda3/lib/python3.8/site-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/lib/python3.8/site-packages (from torch) (3.0)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.8/site-packages (from torch) (3.10.0)\n",
      "Requirement already satisfied: typing-extensions in /root/miniconda3/lib/python3.8/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: sympy in /root/miniconda3/lib/python3.8/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/lib/python3.8/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: lit in /root/miniconda3/lib/python3.8/site-packages (from triton==2.0.0->torch) (15.0.7)\n",
      "Requirement already satisfied: cmake in /root/miniconda3/lib/python3.8/site-packages (from triton==2.0.0->torch) (3.26.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /root/miniconda3/lib/python3.8/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /root/miniconda3/lib/python3.8/site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /root/miniconda3/lib/python3.8/site-packages (from transformers) (1.24.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /root/miniconda3/lib/python3.8/site-packages (from transformers) (0.17.2)\n",
      "Requirement already satisfied: requests in /root/miniconda3/lib/python3.8/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/lib/python3.8/site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/miniconda3/lib/python3.8/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /root/miniconda3/lib/python3.8/site-packages (from transformers) (4.61.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: fsspec in /root/miniconda3/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.9.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/lib/python3.8/site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.8/site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.8/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /root/miniconda3/lib/python3.8/site-packages (from requests->transformers) (1.26.6)\n",
      "Requirement already satisfied: mpmath>=0.19 in /root/miniconda3/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch transformers\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "# lblEncode = {j:i for i,j in zip(range(len(train_df['tag'].unique())), train_df['tag'].unique())}\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-chinese',num_labels=len(lblEncode))\n",
    "# inputs = tokenizer(text, return_tensors='pt', padding=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088a11a6-e34e-4fa0-862a-7722ade9c64d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reverse_lblEncode = {lblEncode[i]:i for i in lblEncode}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcf35ae8-9d7c-46d6-8d49-fabafeee289a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import sklearn.model_selection\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9fa5b36-2ed3-4df7-b5f0-1bbd99f46bd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '本地旅游',\n",
       " 1: '通报查处',\n",
       " 2: '基建民生',\n",
       " 3: '社会热点',\n",
       " 4: '暖新闻',\n",
       " 5: '人事任免',\n",
       " 6: '政策类型',\n",
       " 7: '产业金融',\n",
       " 8: '人文历史',\n",
       " 9: '数据排名'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_lblEncode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61c3aa05-fc25-4ecd-b4de-6adccaf940ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reverse_lblEncode = {lblEncode[i]:i for i in lblEncode}\n",
    "\n",
    "class BERTDataset(Dataset):\n",
    "\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.inputs.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_texts, val_texts, test_texts = train_df['content'].tolist(), val_df['content'].tolist(), test_df['content'].tolist()\n",
    "train_labels, val_labels, test_labels = train_df['tag'].tolist(), val_df['tag'].tolist(), test_df['tag'].tolist()\n",
    "\n",
    "# train_texts, val_texts, test_texts = train_df['content'].iloc[:100].tolist(), val_df['content'].iloc[:100].tolist(), test_df['content'].iloc[:100].tolist()\n",
    "# train_labels, val_labels, test_labels = train_df['tag'].iloc[:100].tolist(), val_df['tag'].iloc[:100].tolist(), test_df['tag'].iloc[:100].tolist()\n",
    "def generateDataloader(df, lbl, BATCH_SIZE = 16):\n",
    "    inputs = tokenizer(df, padding=True, truncation=True,return_tensors='pt')\n",
    "    dataset = BERTDataset(inputs, lbl)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "    return dataloader\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    final_pred = []\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    pred = {i:[] for i in range(len(lblEncode))}\n",
    "    # pred = {i:[] for i in [0,2,3]}\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch in dataloader:\n",
    "            inputs = torch.stack([t for t in batch['input_ids']]).to(device)\n",
    "            labels = torch.tensor(batch['labels']).to(device)\n",
    "            attention_mask =torch.stack([t for t in batch['attention_mask']]).to(device)\n",
    "            outputs = model(input_ids=inputs, attention_mask=attention_mask,\n",
    "                      labels=labels)\n",
    "            predictions = outputs[1].argmax(dim=1)\n",
    "            for i in range(len(predictions)):\n",
    "                predRes = predictions[i].item()\n",
    "                pred[predRes].append((predictions[i] == labels[i]).item())\n",
    "                final_pred.append((reverse_lblEncode[predRes], reverse_lblEncode[labels[i].item()]))\n",
    "                final_pred.append((predRes, labels[i].item()))\n",
    "\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.shape[0]\n",
    "      \n",
    "    accuracy = correct/total\n",
    "    res = {reverse_lblEncode[i]:sum(pred[i])/len(pred[i]) if len(pred[i]) > 0 else np.nan for i in pred }\n",
    "  # res = {i:sum(pred[i])/len(pred[i]) if len(pred[i]) > 0 else np.nan for i in pred }\n",
    "\n",
    "    return accuracy, res, final_pred\n",
    "\n",
    "def model_train(model, train_dataloader, val_dataloader, iter = 3):\n",
    "    for epoch in range(3):\n",
    "      start = time.time()\n",
    "      losses = 0\n",
    "      model.train()\n",
    "      idx = 0\n",
    "      for batch in train_dataloader:\n",
    "        inputs, token_type_ids, attention_mask, labels = batch['input_ids'], batch['token_type_ids'], batch['attention_mask'], batch['labels']\n",
    "\n",
    "        input_ids = torch.stack([t for t in batch['input_ids']]).to(device)\n",
    "\n",
    "        attention_mask =torch.stack([t for t in batch['attention_mask']]).to(device)\n",
    "        labels = torch.tensor(batch['labels']).to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask,labels=labels)\n",
    "        loss = criterion(outputs['logits'],labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        losses += loss.item()\n",
    "        acc = (outputs['logits'].argmax(1) == labels).float().mean()\n",
    "        idx += 1\n",
    "        curr = time.time()\n",
    "        timeConsume = curr - start\n",
    "        if idx % 300 == 0:\n",
    "            print (f'Epoch {epoch+1}, batch {idx}, so far takes {timeConsume}')\n",
    "          # los  =outputs['loss']\n",
    "        print (f'Loss: {losses}')\n",
    "\n",
    "      print (acc)\n",
    "      val_acc, val_cat_acc, val_final_pred= evaluate(model, val_dataloader)\n",
    "      print ('val acc:', val_acc)\n",
    "        # if idx % 10 == 0:\n",
    "        #     logging.info(f\"Epoch: {epoch}, Batch[{idx}/{len(train_iter)}], \"\n",
    "        #                   f\"Train loss :{loss.item():.3f}, Train acc: {acc:.3f}\")\n",
    "\n",
    "      print(f'Epoch {epoch+1} complete, so far takes {timeConsume}')\n",
    "    return model\n",
    "\n",
    "\n",
    "def save_model(model, datPath, model_name = 'BertOrigmodelAllDat_v0.pt'):\n",
    "    # datPath = '/root/wt/'\n",
    "    # model_save_path  = datPath + 'BertOrigmodelAllDat_v0.pt'\n",
    "    model_save_path  = datPath + model_name\n",
    "    if os.path.exists(model_save_path):\n",
    "        loaded_paras = torch.load(model_save_path)\n",
    "        model.load_state_dict(loaded_paras)\n",
    "        logging.info(\"## 成功载入已有模型，进行追加训练......\")\n",
    "    else:\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb49b314-7372-498c-a1f6-7baf62707c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "44a6a14d-a17c-4434-92bb-1ed1c3a31209",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = save_model(model, datPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d003a6b-c0d2-4d4c-967f-eebb7b173a38",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Retrain on the new training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969ddf5a-4e12-4535-a328-4211b70f6341",
   "metadata": {},
   "source": [
    "## Prepare for new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f55fcb9-be5a-4ede-949b-d508fc21fb49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader = generateDataloader(train_df['content'].tolist(), train_df['tag'].tolist()) \n",
    "val_dataloader = generateDataloader(val_df['content'].tolist(), val_df['tag'].tolist()) \n",
    "test_dataloader = generateDataloader(test_df['content'].tolist(), test_df['tag'].tolist()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a15d9af5-eefb-4baa-ab4c-7e62c6d752f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.08729007633587786 Train Cat Accuracy: {'本地旅游': 0.06707317073170732, '通报查处': 0.0, '基建民生': 0.061068702290076333, '社会热点': 0.6525522041763341, '暖新闻': 0.0, '人事任免': 0.038632045598480054, '政策类型': 0.08110578021475896, '产业金融': 0.040842696629213485, '人文历史': 0.0, '数据排名': nan}\n"
     ]
    }
   ],
   "source": [
    "train_accuracy, train_cat_acc, train_pred = evaluate(model, train_dataloader)\n",
    "print(\"Train Accuracy:\", train_accuracy, \"Train Cat Accuracy:\", train_cat_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d957f7f7-0b3a-48a4-9538-bf99a86ae265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, batch 300, so far takes 102.42019772529602\n",
      "Loss: 208.79968886822462\n",
      "Epoch 1, batch 600, so far takes 226.628732919693\n",
      "Loss: 339.1886717826128\n",
      "Epoch 1, batch 900, so far takes 422.81718039512634\n",
      "Loss: 446.6454291306436\n",
      "Epoch 1, batch 1200, so far takes 619.0887923240662\n",
      "Loss: 550.2644704021513\n",
      "Epoch 1, batch 1500, so far takes 815.4085488319397\n",
      "Loss: 636.8789555840194\n",
      "tensor(0.8750, device='cuda:0')\n",
      "val acc: 0.8778812395054191\n",
      "Epoch 1 complete, so far takes 905.3765459060669\n",
      "Epoch 2, batch 300, so far takes 196.26690912246704\n",
      "Loss: 95.35405998118222\n"
     ]
    }
   ],
   "source": [
    "new_model = model_train(model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7965458a-6ab4-4742-b5cb-3cb07dbf0c33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
